{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eGZIh3o1xch"
      },
      "source": [
        "# GEMM on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IcE4c-V5Psg"
      },
      "source": [
        "## 1. Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VkqX6TEG5tz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7601a931-1f2d-4eb3-8357-02484ff3f85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Collecting tlcpack-nightly-cu102\n",
            "  Downloading https://github.com/tlc-pack/tlcpack/releases/download/v0.12.dev/tlcpack_nightly_cu102-0.15.dev118%2Bg51bdaec6e-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m428.5/428.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (25.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (2.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (1.14.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (6.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.12.2)\n",
            "Installing collected packages: tlcpack-nightly-cu102\n",
            "Successfully installed tlcpack-nightly-cu102-0.15.dev118+g51bdaec6e\n",
            "Collecting numpy<2.0.0\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install tlcpack-nightly-cu102 -f https://tlcpack.ai/wheels\n",
        "!pip install \"numpy<2.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GZ_We05Psj"
      },
      "source": [
        "## 3. Check the implementation of `make_gemm_gpu_scheduler` function in `src.ops`\n",
        "\n",
        "The function implements General Matrix Multiply (GEMM) on GPU. You should use TVM to optimize it.\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times k}$, $W \\in \\mathbb{R}^{k \\times n}$, and $B \\in \\mathbb{R}^{m \\times n}$, then\n",
        "$$\n",
        "B = A \\times W\n",
        "$$\n",
        "Please see the numpy matmul function for more detail: [link](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
        "\n",
        "The `make_gemm_gpu_scheduler` takes $m$, $k$, and $n$. The first matrix is $m \\times k$, the second matrix is $k \\times n$, and the output matrix is $m \\times n$.\n",
        "\n",
        "The function returns both the TVM scheduler and the TVM opterator for\n",
        "1. Input $a$\n",
        "2. Input $w$\n",
        "3. Output $b$\n",
        "\n",
        "The scheduler should be able to used to build a function with signature $func(a, w, b)$.\n",
        "Please see the following cells for usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tvm\n",
        "import numpy as np\n",
        "from tvm import te\n",
        "\n",
        "# benchmark for tvm implementation\n",
        "def benchmark_gemm_tvm(schedule_func, M, K, N, device, a_np, w_np, num_runs=30, repeat=20):\n",
        "    s, A, W, B = schedule_func(M, K, N)\n",
        "    func = tvm.build(s, [A, W, B], target=\"cuda\")\n",
        "\n",
        "    dev = tvm.cuda(0)\n",
        "    a = tvm.nd.array(a_np, dev)\n",
        "    w = tvm.nd.array(w_np, dev)\n",
        "    out_tvm = tvm.nd.array(np.zeros((M, N), dtype), dev)\n",
        "    evaluator = func.time_evaluator(func.entry_name, dev, number=num_runs, repeat=repeat)\n",
        "    cost = evaluator(a, w, out_tvm).mean\n",
        "\n",
        "    return cost, out_tvm.asnumpy(), func, (s, A, W, B)\n",
        "\n",
        "def base_declaration(M, K, N):\n",
        "    k = te.reduce_axis((0, K), \"k\")\n",
        "    A = te.placeholder((M, K), name=\"A\")\n",
        "    B = te.placeholder((K, N), name=\"B\")\n",
        "    C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name=\"C\")\n",
        "    s = te.create_schedule(C.op)\n",
        "    return k, s, A, B, C\n",
        "\n",
        "M = 1024\n",
        "N = 512\n",
        "K = 2048\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M, K).astype(dtype)\n",
        "w_np = np.random.rand(K, N).astype(dtype)\n",
        "ref = np.matmul(a_np, w_np)\n"
      ],
      "metadata": {
        "id": "e8ldlOVJDWbv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gemm_gpu_scheduler_naive(M, K, N, verbose=True):\n",
        "    k, s, A, B, C = base_declaration(M, K, N)\n",
        "\n",
        "    # overall index of a thread: ùëñ=blockIdx.x√óblockDim.x+threadIdx.x\n",
        "    block_x = te.thread_axis(\"blockIdx.y\")\n",
        "    block_y = te.thread_axis(\"blockIdx.x\")\n",
        "\n",
        "    x, y = s[C].op.axis\n",
        "    (k,) = s[C].op.reduce_axis\n",
        "    s[C].bind(y, block_y)\n",
        "    s[C].bind(x, block_x)\n",
        "    if verbose:\n",
        "        print(\"=\" * 100)\n",
        "        print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
        "        print(\"=\" * 100)\n",
        "    return s, A, B, C\n",
        "\n",
        "# naive TVM\n",
        "dev = tvm.cuda()\n",
        "naive_time, naive_res, naive_func, naive_comp = benchmark_gemm_tvm(\n",
        "    make_gemm_gpu_scheduler_naive, M, K, N, dev, a_np, w_np, num_runs=5, repeat=5\n",
        ")\n",
        "np.testing.assert_allclose(naive_res, ref, rtol=1e-4)\n",
        "print(f\"[TVM Naive] time: {naive_time*1e3:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Giii5g8_E8lP",
        "outputId": "5b0abb4d-5755-44ba-9023-209c4f249687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 512), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_y = T.launch_thread(\"blockIdx.y\", 1024)\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 512)\n",
            "        C_1 = T.Buffer((524288,), data=C.data)\n",
            "        C_1[blockIdx_y * 512 + blockIdx_x] = T.float32(0)\n",
            "        for k in range(2048):\n",
            "            A_1 = T.Buffer((2097152,), data=A.data)\n",
            "            B_1 = T.Buffer((1048576,), data=B.data)\n",
            "            C_1[blockIdx_y * 512 + blockIdx_x] = C_1[blockIdx_y * 512 + blockIdx_x] + A_1[blockIdx_y * 2048 + k] * B_1[k * 512 + blockIdx_x]\n",
            "====================================================================================================\n",
            "[TVM Naive] time: 84.5233 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opt v1: tiling + threads 1D\n",
        "def make_gemm_gpu_scheduler_v1(M, K, N, verbose=True):\n",
        "    k, s, A, B, C = base_declaration(M, K, N)\n",
        "\n",
        "    x, y = s[C].op.axis\n",
        "\n",
        "    # split the axes\n",
        "    xo, xi = s[C].split(x, factor=32)\n",
        "\n",
        "    # bind the outer axes to blocks\n",
        "    s[C].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[C].bind(y, te.thread_axis(\"blockIdx.y\"))\n",
        "\n",
        "    # bind the inner axes to threads\n",
        "    s[C].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 100)\n",
        "        print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "    return s, A, B, C\n",
        "\n",
        "# Test the v1 optimization\n",
        "dev = tvm.cuda()\n",
        "v1_time, v1_res, v1_func, v1_comp = benchmark_gemm_tvm(\n",
        "    make_gemm_gpu_scheduler_v1, M, K, N, dev, a_np, w_np, num_runs=20, repeat=20\n",
        ")\n",
        "np.testing.assert_allclose(v1_res, ref, rtol=1e-4)\n",
        "print(f\"[TVM v1] time: {v1_time*1e3:.4f} ms\")"
      ],
      "metadata": {
        "id": "0gP9SZYQGkJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc0912d-2e0e-43a7-850d-0b888c3519d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 512), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 32)\n",
            "        threadIdx_x = T.launch_thread(\"threadIdx.x\", 32)\n",
            "        blockIdx_y = T.launch_thread(\"blockIdx.y\", 512)\n",
            "        C_1 = T.Buffer((524288,), data=C.data)\n",
            "        C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] = T.float32(0)\n",
            "        for k in range(2048):\n",
            "            A_1 = T.Buffer((2097152,), data=A.data)\n",
            "            B_1 = T.Buffer((1048576,), data=B.data)\n",
            "            C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] = C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] + A_1[blockIdx_x * 65536 + threadIdx_x * 2048 + k] * B_1[k * 512 + blockIdx_y]\n",
            "====================================================================================================\n",
            "[TVM v1] time: 36.9797 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opt v2: tiling + threads 2D\n",
        "def make_gemm_gpu_scheduler_v2(M, K, N, verbose=True):\n",
        "    k, s, A, B, C = base_declaration(M, K, N)\n",
        "\n",
        "    x, y = s[C].op.axis\n",
        "\n",
        "    # split the axes\n",
        "    xo, xi = s[C].split(x, factor=32)\n",
        "    yo, yi = s[C].split(y, factor=32)\n",
        "\n",
        "    # bind the outer axes to blocks\n",
        "    s[C].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[C].bind(yo, te.thread_axis(\"blockIdx.y\"))\n",
        "\n",
        "    # bind the inner axes to threads\n",
        "    s[C].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "    s[C].bind(yi, te.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 100)\n",
        "        print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "    return s, A, B, C\n",
        "\n",
        "dev = tvm.cuda()\n",
        "time, res, func, comp = benchmark_gemm_tvm(\n",
        "    make_gemm_gpu_scheduler_v2, M, K, N, dev, a_np, w_np, num_runs=20, repeat=20\n",
        ")\n",
        "np.testing.assert_allclose(res, ref, rtol=1e-4)\n",
        "print(f\"[TVM v2] time: {time*1e3:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGxn-Ygvd7xI",
        "outputId": "54ca0cc7-03c3-4005-a4d0-21f6800cdb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 512), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 32)\n",
            "        threadIdx_x = T.launch_thread(\"threadIdx.x\", 32)\n",
            "        blockIdx_y = T.launch_thread(\"blockIdx.y\", 16)\n",
            "        threadIdx_y = T.launch_thread(\"threadIdx.y\", 32)\n",
            "        C_1 = T.Buffer((524288,), data=C.data)\n",
            "        C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = T.float32(0)\n",
            "        for k in range(2048):\n",
            "            A_1 = T.Buffer((2097152,), data=A.data)\n",
            "            B_1 = T.Buffer((1048576,), data=B.data)\n",
            "            C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] + A_1[blockIdx_x * 65536 + threadIdx_x * 2048 + k] * B_1[k * 512 + blockIdx_y * 32 + threadIdx_y]\n",
            "====================================================================================================\n",
            "[TVM v2] time: 35.5007 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opt3: v2 + cache (with multi threads)\n",
        "def make_gemm_gpu_scheduler_v3(M, K, N, verbose=True):\n",
        "    k, s, A, B, C = base_declaration(M, K, N)\n",
        "    block_x, block_y = 16, 16\n",
        "    xo, xi = s[C].split(C.op.axis[0], factor=block_x)\n",
        "    yo, yi = s[C].split(C.op.axis[1], factor=block_y)\n",
        "\n",
        "    # split k\n",
        "    tile_k = 8\n",
        "    ko, ki = s[C].split(k, factor=tile_k)\n",
        "\n",
        "    s[C].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[C].bind(yo, te.thread_axis(\"blockIdx.y\"))\n",
        "    s[C].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "    s[C].bind(yi, te.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "    AA = s.cache_read(A, \"shared\", [C])\n",
        "    BB = s.cache_read(B, \"shared\", [C])\n",
        "\n",
        "    s[AA].compute_at(s[C], ko)\n",
        "    s[BB].compute_at(s[C], ko)\n",
        "\n",
        "    # multi threads for loading data\n",
        "    # this increases performance a lot!\n",
        "    AAxi, AAyi = s[AA].split(s[AA].op.axis[0], nparts=block_x)\n",
        "    AAxx, AAxy = s[AA].split(s[AA].op.axis[1], nparts=block_y)\n",
        "    s[AA].bind(AAxi, te.thread_axis(\"threadIdx.x\"))\n",
        "    s[AA].bind(AAxx, te.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "    BBxi, BByi = s[BB].split(s[BB].op.axis[0], nparts=block_x)\n",
        "    BBxx, BBxy = s[BB].split(s[BB].op.axis[1], nparts=block_y)\n",
        "    s[BB].bind(BBxi, te.thread_axis(\"threadIdx.x\"))\n",
        "    s[BB].bind(BBxx, te.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 100)\n",
        "        print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "    return s, A, B, C\n",
        "\n",
        "\n",
        "dev = tvm.cuda()\n",
        "time, res, func, comp = benchmark_gemm_tvm(\n",
        "    make_gemm_gpu_scheduler_v3, M, K, N, dev, a_np, w_np, num_runs=20, repeat=20\n",
        ")\n",
        "np.testing.assert_allclose(res, ref, rtol=1e-4)\n",
        "print(f\"[TVM v3] time: {time*1e3:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTCTuDABajYQ",
        "outputId": "5c043ad0-33da-4373-b996-f806e410f0e6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((1024, 2048), \"float32\"), B: T.Buffer((2048, 512), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 64)\n",
            "        A_shared = T.allocate([128], \"float32\", \"shared\")\n",
            "        B_shared = T.allocate([128], \"float32\", \"shared\")\n",
            "        threadIdx_x = T.launch_thread(\"threadIdx.x\", 16)\n",
            "        blockIdx_y = T.launch_thread(\"blockIdx.y\", 32)\n",
            "        threadIdx_y = T.launch_thread(\"threadIdx.y\", 16)\n",
            "        C_1 = T.Buffer((524288,), data=C.data)\n",
            "        C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] = T.float32(0)\n",
            "        for k_outer in range(256):\n",
            "            A_shared_1 = T.Buffer((128,), data=A_shared, scope=\"shared\")\n",
            "            with T.launch_thread(\"threadIdx.x\", 16) as threadIdx_x_1:\n",
            "                threadIdx_y_1 = T.launch_thread(\"threadIdx.y\", 16)\n",
            "                if T.likely(threadIdx_y_1 < 8):\n",
            "                    A_1 = T.Buffer((2097152,), data=A.data)\n",
            "                    A_shared_1[threadIdx_x_1 * 8 + threadIdx_y_1] = A_1[blockIdx_x * 32768 + threadIdx_x_1 * 2048 + k_outer * 8 + threadIdx_y_1]\n",
            "            B_shared_1 = T.Buffer((128,), data=B_shared, scope=\"shared\")\n",
            "            with T.launch_thread(\"threadIdx.x\", 16) as threadIdx_x_1:\n",
            "                threadIdx_y_1 = T.launch_thread(\"threadIdx.y\", 16)\n",
            "                if T.likely(threadIdx_x_1 < 8):\n",
            "                    B_1 = T.Buffer((1048576,), data=B.data)\n",
            "                    B_shared_1[threadIdx_x_1 * 16 + threadIdx_y_1] = B_1[k_outer * 4096 + threadIdx_x_1 * 512 + blockIdx_y * 16 + threadIdx_y_1]\n",
            "            for k_inner in range(8):\n",
            "                C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] = C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] + A_shared_1[threadIdx_x * 8 + k_inner] * B_shared_1[k_inner * 16 + threadIdx_y]\n",
            "====================================================================================================\n",
            "[TVM v3] time: 8.1074 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import te, autotvm\n",
        "import numpy as np\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "@autotvm.template(\"gemm_gpu_autotvm\")\n",
        "def gemm_gpu(M, K, N, dtype=\"float32\"):\n",
        "    k, s, A, B, C = base_declaration(M, K, N)\n",
        "\n",
        "    # Important: cache_write must be placed before any other transformations\n",
        "    local_C = s.cache_write(C, \"local\")\n",
        "\n",
        "    x, y = s[C].op.axis\n",
        "    cfg = autotvm.get_config()\n",
        "\n",
        "    cfg.define_knob(\"tile_x\", [8, 16, 32])\n",
        "    cfg.define_knob(\"tile_y\", [8, 16, 32])\n",
        "    cfg.define_knob(\"tile_k\", [8, 16])\n",
        "\n",
        "    xo, xi = s[C].split(x, factor=cfg[\"tile_x\"].val)\n",
        "    yo, yi = s[C].split(y, factor=cfg[\"tile_y\"].val)\n",
        "\n",
        "\n",
        "    s[C].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[C].bind(yo, te.thread_axis(\"blockIdx.y\"))\n",
        "    s[C].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "    s[C].bind(yi, te.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "    s[local_C].compute_at(s[C], yi)\n",
        "    k = s[local_C].op.reduce_axis[0]\n",
        "    ko, ki = s[local_C].split(k, factor=cfg[\"tile_k\"].val)\n",
        "\n",
        "    AA = s.cache_read(A, \"shared\", [local_C])\n",
        "    BB = s.cache_read(B, \"shared\", [local_C])\n",
        "\n",
        "    s[AA].compute_at(s[local_C], ko)\n",
        "    s[BB].compute_at(s[local_C], ko)\n",
        "\n",
        "    cfg.define_knob(\"vectorize\", [0, 1])\n",
        "    if cfg[\"vectorize\"].val:\n",
        "        s[AA].vectorize(s[AA].op.axis[-1])\n",
        "        s[BB].vectorize(s[BB].op.axis[-1])\n",
        "\n",
        "    return s, [A, B, C]\n",
        "\n",
        "\n",
        "\n",
        "task = autotvm.task.create(\"gemm_gpu_autotvm\", args=(M, K, N, dtype), target=\"cuda\")\n",
        "print(\"Search space size:\", len(task.config_space))\n",
        "\n",
        "log_file = \"gemm_gpu_autotvm.log\"\n",
        "tuner = autotvm.tuner.RandomTuner(task)\n",
        "measure_option = autotvm.measure_option(\n",
        "    builder=autotvm.LocalBuilder(),\n",
        "    runner=autotvm.LocalRunner(repeat=3, min_repeat_ms=50, timeout=4)\n",
        ")\n",
        "num_trials = 30\n",
        "tuner.tune(\n",
        "    n_trial=num_trials,\n",
        "    measure_option=measure_option,\n",
        "    callbacks=[autotvm.callback.log_to_file(log_file)],\n",
        ")\n",
        "\n",
        "# apply the best configuration\n",
        "with autotvm.apply_history_best(log_file):\n",
        "    with tvm.target.Target(\"cuda\"):\n",
        "        s, args = gemm_gpu(M, K, N, dtype)\n",
        "        func = tvm.build(s, args)\n",
        "\n",
        "# verify and benchmark\n",
        "a_np = np.random.rand(M, K).astype(dtype)\n",
        "b_np = np.random.rand(K, N).astype(dtype)\n",
        "c_np = np.zeros((M, N), dtype=dtype)\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a_tvm = tvm.nd.array(a_np, dev)\n",
        "b_tvm = tvm.nd.array(b_np, dev)\n",
        "c_tvm = tvm.nd.array(c_np, dev)\n",
        "\n",
        "func(a_tvm, b_tvm, c_tvm)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=10, repeat=10)\n",
        "time_cost = evaluator(a_tvm, b_tvm, c_tvm).mean * 1000  # convert to ms\n",
        "print(f\"AutoTVM tuned GEMM execution time: {time_cost:.4f} ms\")\n",
        "c_ref = np.matmul(a_np, b_np)\n",
        "np.testing.assert_allclose(c_tvm.asnumpy(), c_ref, rtol=1e-4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yejZoS5Qsj1Y",
        "outputId": "2bf9f377-0842-4e9a-ee4d-6f845ff5f744"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space size: 36\n",
            "AutoTVM tuned GEMM execution time: 42.5615 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark for numpy\n",
        "def benchmark_gemm_numpy(a_np, w_np, num_runs=10):\n",
        "    t0 = time.time()\n",
        "    out = None\n",
        "    for _ in range(num_runs):\n",
        "        out = np.matmul(a_np, w_np)\n",
        "    t1 = time.time()\n",
        "    return (t1 - t0) / num_runs, out\n",
        "\n",
        "# numPy baseline\n",
        "numpy_time, numpy_out = benchmark_gemm_numpy(a_np, w_np, num_runs=10)\n",
        "print(f\"[NumPy]   time: {numpy_time*1e3:.4f} ms\")\n",
        "np.testing.assert_allclose(numpy_out, ref, rtol=1e-4)\n"
      ],
      "metadata": {
        "id": "yF_2oiPVGkMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c9aa3d-043b-49d8-d34e-4da6debd0313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NumPy]   time: 74.9496 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def benchmark_gemm_torch(M, K, N, a_np, w_np, num_runs=30, repeat=20, device=\"cuda\"):\n",
        "    a_torch = torch.tensor(a_np, dtype=torch.float32).to(device)\n",
        "    w_torch = torch.tensor(w_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    for _ in range(10):\n",
        "        _ = torch.matmul(a_torch, w_torch)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    times = []\n",
        "    for r in range(repeat):\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_runs):\n",
        "            out = torch.matmul(a_torch, w_torch)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        times.append((end_time - start_time) / num_runs)\n",
        "\n",
        "    avg_time = sum(times) / len(times)\n",
        "\n",
        "    out_np = out.cpu().numpy()\n",
        "    return avg_time, out_np\n",
        "\n",
        "# torch cpu\n",
        "cpu_time, cpu_res = benchmark_gemm_torch(M, K, N, a_np, w_np,\n",
        "                                         num_runs=5, repeat=5, device=\"cpu\")\n",
        "np.testing.assert_allclose(cpu_res, ref, rtol=1e-4)\n",
        "print(f\"[PyTorch CPU] time: {cpu_time*1e3:.4f} ms\")\n",
        "\n",
        "# torch gpu\n",
        "gpu_time, gpu_res = benchmark_gemm_torch(M, K, N, a_np, w_np,\n",
        "                                        num_runs=20, repeat=20, device=\"cuda\")\n",
        "np.testing.assert_allclose(gpu_res, ref, rtol=1e-4)\n",
        "print(f\"[PyTorch CUDA] time: {gpu_time*1e3:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F---DqIAoJXQ",
        "outputId": "1c9194bd-7f86-4be5-f1e7-1d365d703980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PyTorch CPU] time: 18.7425 ms\n",
            "[PyTorch CUDA] time: 0.7027 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQiasz7n5Psk",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82064646-6055-4bfc-c2f9-781515110bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipytest\n",
            "  Downloading ipytest-0.14.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from ipytest) (7.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipytest) (24.2)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.11/dist-packages (from ipytest) (8.3.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=5.4->ipytest) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=5.4->ipytest) (1.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipytest)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->ipytest) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->ipytest) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.13)\n",
            "Downloading ipytest-0.14.2-py3-none-any.whl (18 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipytest\n",
            "Successfully installed ipytest-0.14.2 jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "# pytest\n",
        "%pip install ipytest\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest\n",
        "import tvm\n",
        "import torch\n",
        "import pytest\n",
        "import timeit\n",
        "import numpy as np\n",
        "\n",
        "make_gemm_gpu_scheduler = make_gemm_gpu_scheduler_v3\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "\n",
        "def ans_np(a, b):\n",
        "    return np.matmul(a, b)\n",
        "\n",
        "\n",
        "def make_func(M, K, N):\n",
        "    s, A, B, O = make_gemm_gpu_scheduler(M, K, N)\n",
        "    func = tvm.build(s, [A, B, O], \"cuda\")\n",
        "    return func\n",
        "\n",
        "\n",
        "def ans_torch(a_torch, b_torch):\n",
        "    torch.cuda.synchronize()\n",
        "    out = torch.mm(a_torch, b_torch)\n",
        "    torch.cuda.synchronize()\n",
        "    return out\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('execution_number', range(5))\n",
        "def test1_M1_N1_K2(execution_number):\n",
        "    # Define dimension\n",
        "    M = 1\n",
        "    N = 1\n",
        "    K = 2\n",
        "    func = make_func(M, K, N)\n",
        "\n",
        "    # Create random test data\n",
        "    np.random.seed(seed=execution_number)\n",
        "    a_np = np.random.rand(M, K).astype(np.float32)\n",
        "    w_np = np.random.rand(K, N).astype(np.float32)\n",
        "    b_np = ans_np(a_np, w_np)\n",
        "\n",
        "    a = tvm.nd.array(a_np, dev)\n",
        "    w = tvm.nd.array(w_np, dev)\n",
        "    b = tvm.nd.array(np.zeros((M, N), dtype='float32'), dev)\n",
        "    func(a, w, b)\n",
        "    b_out = b.numpy()\n",
        "\n",
        "    assert b_np.shape == b_out.shape, \\\n",
        "        \"Shape mismatch: \" + str(b_np.shape) + \"\\t\" + str(b_out.shape)\n",
        "    assert np.allclose(b_np, b_out), \"Value mismatch: %s %s\" % (b_np, b_out)\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('execution_number', [1, 10, 100, 1000, 10000])\n",
        "def test1_Mvar_N1024_K1024(execution_number):\n",
        "    # Define dimension\n",
        "    M = execution_number\n",
        "    N = 1024\n",
        "    K = 1024\n",
        "    func = make_func(M, K, N)\n",
        "\n",
        "    # Create random test data\n",
        "    np.random.seed(seed=1024)\n",
        "    a_np = np.random.rand(M, K).astype(np.float32)\n",
        "    w_np = np.random.rand(K, N).astype(np.float32)\n",
        "    b_np = ans_np(a_np, w_np)\n",
        "\n",
        "    a = tvm.nd.array(a_np, dev)\n",
        "    w = tvm.nd.array(w_np, dev)\n",
        "    b = tvm.nd.array(np.zeros((M, N), dtype='float32'), dev)\n",
        "    func(a, w, b)\n",
        "    b_out = b.numpy()\n",
        "\n",
        "    assert b_np.shape == b_out.shape, \\\n",
        "        \"Shape mismatch: \" + str(b_np.shape) + \"\\t\" + str(b_out.shape)\n",
        "    assert np.allclose(b_np, b_out), \"Value mismatch: %s %s\" % (b_np, b_out)\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('execution_number', [1, 10, 100, 1000, 10000])\n",
        "def test1_M1024_Nvar_K1024(execution_number):\n",
        "    # Define dimension\n",
        "    M = 1024\n",
        "    N = execution_number\n",
        "    K = 1024\n",
        "    func = make_func(M, K, N)\n",
        "\n",
        "    # Create random test data\n",
        "    np.random.seed(seed=1024)\n",
        "    a_np = np.random.rand(M, K).astype(np.float32)\n",
        "    w_np = np.random.rand(K, N).astype(np.float32)\n",
        "    b_np = ans_np(a_np, w_np)\n",
        "\n",
        "    a = tvm.nd.array(a_np, dev)\n",
        "    w = tvm.nd.array(w_np, dev)\n",
        "    b = tvm.nd.array(np.zeros((M, N), dtype='float32'), dev)\n",
        "    func(a, w, b)\n",
        "    b_out = b.numpy()\n",
        "\n",
        "    assert b_np.shape == b_out.shape, \\\n",
        "        \"Shape mismatch: \" + str(b_np.shape) + \"\\t\" + str(b_out.shape)\n",
        "    assert np.allclose(b_np, b_out), \"Value mismatch: %s %s\" % (b_np, b_out)\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('execution_number', [1, 10, 100, 1000, 10000])\n",
        "def test1_M1024_N1024_Kvar(execution_number):\n",
        "    # Define dimension\n",
        "    M = 1024\n",
        "    N = 1024\n",
        "    K = execution_number\n",
        "    func = make_func(M, K, N)\n",
        "\n",
        "    # Create random test data\n",
        "    np.random.seed(seed=1024)\n",
        "    a_np = np.random.rand(M, K).astype(np.float32)\n",
        "    w_np = np.random.rand(K, N).astype(np.float32)\n",
        "    b_np = ans_np(a_np, w_np)\n",
        "\n",
        "    a = tvm.nd.array(a_np, dev)\n",
        "    w = tvm.nd.array(w_np, dev)\n",
        "    b = tvm.nd.array(np.zeros((M, N), dtype='float32'), dev)\n",
        "    func(a, w, b)\n",
        "    b_out = b.numpy()\n",
        "\n",
        "    assert b_np.shape == b_out.shape, \\\n",
        "        \"Shape mismatch: \" + str(b_np.shape) + \"\\t\" + str(b_out.shape)\n",
        "    assert np.allclose(b_np, b_out), \"Value mismatch: %s %s\" % (b_np, b_out)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hPSaxTXDCjQ",
        "outputId": "b8e5a36d-2377-4d57-9bbf-ed136204899a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m20 passed\u001b[0m\u001b[32m in 19.22s\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}