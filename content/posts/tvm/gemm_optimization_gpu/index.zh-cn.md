---
title: "TVM: GEMM GPU Optimization"
date: 2025-04-06T17:06:56+08:00
categories: ["tvm"]
summary: "æœ¬åšå®¢å±•ç¤ºäº†ä½¿ç”¨ TVM åœ¨ GPU ä¸Šä¼˜åŒ– GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬çº¿ç¨‹ç»„ç»‡å’Œå†…å­˜å±‚æ¬¡ç»“æ„åˆ©ç”¨"
---

> æœ¬åšå®¢ä½¿ç”¨`claude-3.7-sonet`ç¿»è¯‘ï¼Œå¦‚æœ‰å†²çªè¯·ä¼˜å…ˆå‚è€ƒè‹±æ–‡åŸæ–‡

## æ‘˜è¦

æœ¬åšå®¢å±•ç¤ºäº†ä½¿ç”¨ TVM åœ¨ GPU ä¸Šä¼˜åŒ– GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬çº¿ç¨‹ç»„ç»‡å’Œå†…å­˜å±‚æ¬¡ç»“æ„åˆ©ç”¨ã€‚

## 1. ç¯å¢ƒ

ç¯å¢ƒï¼šGoogle Colab T4 GPU

æˆ‘ä»¬åŸºäºä»¥ä¸‹é…ç½®è¿›è¡Œæµ‹è¯•ï¼š

```py
M = 1024
N = 512
K = 2048
dtype = 'float32'
a_np = np.random.rand(M, K).astype(dtype)
w_np = np.random.rand(K, N).astype(dtype)
ref = np.matmul(a_np, w_np)
```

### 2.1 åŸºå‡†æµ‹è¯•å’Œæ‰‹åŠ¨ä¼˜åŒ–

#### 2.1.1 æœ´ç´ åŸºå‡†

åˆå§‹å®ç°çš„è¿è¡Œæ—¶é—´ä¸º **84.52 æ¯«ç§’**ã€‚

```py
def make_gemm_gpu_scheduler_naive(M, K, N, verbose=True):
    k, s, A, B, C = base_declaration(M, K, N)

    # çº¿ç¨‹çš„æ•´ä½“ç´¢å¼•ï¼šğ‘–=blockIdx.xÃ—blockDim.x+threadIdx.x
    block_x = te.thread_axis("blockIdx.y")
    block_y = te.thread_axis("blockIdx.x")

    x, y = s[C].op.axis
    (k,) = s[C].op.reduce_axis
    s[C].bind(y, block_y)
    s[C].bind(x, block_x)
    if verbose:
        print("=" * 100)
        print(tvm.lower(s, [A, B, C], simple_mode=True))
        print("=" * 100)
    return s, A, B, C
```

IRï¼š

```py
@T.prim_func
def main(A: T.Buffer((1024, 2048), "float32"), B: T.Buffer((2048, 512), "float32"), C: T.Buffer((1024, 512), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 1024)
    blockIdx_x = T.launch_thread("blockIdx.x", 512)
    C_1 = T.Buffer((524288,), data=C.data)
    C_1[blockIdx_y * 512 + blockIdx_x] = T.float32(0)
    for k in range(2048):
        A_1 = T.Buffer((2097152,), data=A.data)
        B_1 = T.Buffer((1048576,), data=B.data)
        # blockIdx_y * 512 + blockIdx_x: è¾“å‡ºä½ç½®
        # blockIdx_y * 2048 + k: A_1 ä½ç½®
        # k * 512 + blockIdx_x: B_1 ä½ç½®
        C_1[blockIdx_y * 512 + blockIdx_x] = C_1[blockIdx_y * 512 + blockIdx_x] + A_1[blockIdx_y * 2048 + k] * B_1[k * 512 + blockIdx_x]
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å£°æ˜äº†ä¸€ä¸ªäºŒç»´å—åŒºåŸŸï¼Œæ¯ä¸ªå—è´Ÿè´£è®¡ç®—ä¸€ä¸ªè¾“å‡ºã€‚è¿™ç§æ–¹æ³•éå¸¸æ…¢ã€‚

#### 2.1.2 v1ï¼šåˆ†å— + ä¸€ç»´çº¿ç¨‹

ç°åœ¨æˆ‘ä»¬å°† x è½´åˆ†å‰²æˆå—å’Œç“¦ç‰‡ï¼Œå°†å¤–éƒ¨éƒ¨åˆ†ç»‘å®šåˆ°å—ï¼Œå†…éƒ¨éƒ¨åˆ†ç»‘å®šåˆ°çº¿ç¨‹ã€‚

```py
# ä¼˜åŒ– v1ï¼šåˆ†å— + ä¸€ç»´çº¿ç¨‹
def make_gemm_gpu_scheduler_v1(M, K, N, verbose=True):
    k, s, A, B, C = base_declaration(M, K, N)

    x, y = s[C].op.axis

    # åˆ†å‰²è½´
    xo, xi = s[C].split(x, factor=32)

    # å°†å¤–éƒ¨è½´ç»‘å®šåˆ°å—
    s[C].bind(xo, te.thread_axis("blockIdx.x"))
    s[C].bind(y, te.thread_axis("blockIdx.y"))

    # å°†å†…éƒ¨è½´ç»‘å®šåˆ°çº¿ç¨‹
    s[C].bind(xi, te.thread_axis("threadIdx.x"))

    if verbose:
        print("=" * 100)
        print(tvm.lower(s, [A, B, C], simple_mode=True))
        print("=" * 100)

    return s, A, B, C
```

IRï¼š

```py
@T.prim_func
def main(A: T.Buffer((1024, 2048), "float32"), B: T.Buffer((2048, 512), "float32"), C: T.Buffer((1024, 512), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 32)
    threadIdx_x = T.launch_thread("threadIdx.x", 32)
    blockIdx_y = T.launch_thread("blockIdx.y", 512)
    C_1 = T.Buffer((524288,), data=C.data)
    C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] = T.float32(0)
    for k in range(2048):
        A_1 = T.Buffer((2097152,), data=A.data)
        B_1 = T.Buffer((1048576,), data=B.data)
        # lockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y: è¾“å‡º
        # blockIdx_x * 65536 + threadIdx_x * 2048 + k: A1
        # k * 512 + blockIdx_y: B1
        C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] = C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y] + A_1[blockIdx_x * 65536 + threadIdx_x * 2048 + k] * B_1[k * 512 + blockIdx_y]
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç»´çº¿ç¨‹æ¶æ„æ¥æ”¯æŒæ›´é«˜æ•ˆçš„å¹¶è¡Œæ€§ã€‚è¿™å°†æ€§èƒ½æé«˜åˆ° **36.98 æ¯«ç§’**ã€‚

#### 2.1.3 v2ï¼šåˆ†å— + äºŒç»´çº¿ç¨‹

åœ¨ v1 çš„åŸºç¡€ä¸Šï¼Œè¯¥ç‰ˆæœ¬å®ç°äºŒç»´çº¿ç¨‹ç»„ç»‡ï¼Œå°† x å’Œ y è½´éƒ½è¿›è¡Œåˆ†å‰²ã€‚è¿™æ ·æ¯ä¸ªå—ä¸­çš„çº¿ç¨‹ä»¥ 32Ã—32 çš„ç½‘æ ¼ç»„ç»‡ï¼Œèƒ½æ›´é«˜æ•ˆåœ°åˆ©ç”¨ GPU èµ„æºã€‚

```py
# ä¼˜åŒ– v2ï¼šåˆ†å— + äºŒç»´çº¿ç¨‹
def make_gemm_gpu_scheduler_v2(M, K, N, verbose=True):
    k, s, A, B, C = base_declaration(M, K, N)

    x, y = s[C].op.axis

    # åˆ†å‰²è½´
    xo, xi = s[C].split(x, factor=32)
    yo, yi = s[C].split(y, factor=32)

    # å°†å¤–éƒ¨è½´ç»‘å®šåˆ°å—
    s[C].bind(xo, te.thread_axis("blockIdx.x"))
    s[C].bind(yo, te.thread_axis("blockIdx.y"))

    # å°†å†…éƒ¨è½´ç»‘å®šåˆ°çº¿ç¨‹
    s[C].bind(xi, te.thread_axis("threadIdx.x"))
    s[C].bind(yi, te.thread_axis("threadIdx.y"))

    if verbose:
        print("=" * 100)
        print(tvm.lower(s, [A, B, C], simple_mode=True))
        print("=" * 100)

    return s, A, B, C

dev = tvm.cuda()
time, res, func, comp = benchmark_gemm_tvm(
    make_gemm_gpu_scheduler_v2, M, K, N, dev, a_np, w_np, num_runs=20, repeat=20
)
np.testing.assert_allclose(res, ref, rtol=1e-4)
print(f"[TVM v2] time: {time*1e3:.4f} ms")
```

IRï¼š

```py
@T.prim_func
def main(A: T.Buffer((1024, 2048), "float32"), B: T.Buffer((2048, 512), "float32"), C: T.Buffer((1024, 512), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 32)
    threadIdx_x = T.launch_thread("threadIdx.x", 32)
    blockIdx_y = T.launch_thread("blockIdx.y", 16)
    threadIdx_y = T.launch_thread("threadIdx.y", 32)
    C_1 = T.Buffer((524288,), data=C.data)
    C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = T.float32(0)
    for k in range(2048):
        A_1 = T.Buffer((2097152,), data=A.data)
        B_1 = T.Buffer((1048576,), data=B.data)
        # blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y: è¾“å‡º
        # blockIdx_x * 65536 + threadIdx_x * 2048 + k: A1
        # k * 512 + blockIdx_y * 32 + threadIdx_y: B1
        C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] = C_1[blockIdx_x * 16384 + threadIdx_x * 512 + blockIdx_y * 32 + threadIdx_y] + A_1[blockIdx_x * 65536 + threadIdx_x * 2048 + k] * B_1[k * 512 + blockIdx_y * 32 + threadIdx_y]
```

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨äºŒç»´çº¿ç¨‹æ¶æ„æ¥è¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚æ€§èƒ½ç•¥æœ‰æå‡è‡³ **35.50 æ¯«ç§’**ã€‚

#### 2.1.4 v3ï¼šå…±äº«å†…å­˜ç¼“å­˜ + å¤šçº¿ç¨‹åŠ è½½

æ­¤ç‰ˆæœ¬åˆ©ç”¨ GPU å†…å­˜å±‚æ¬¡ç»“æ„ï¼š

- å°†è¾“å…¥çŸ©é˜µ `A` å’Œ `B` ç¼“å­˜åœ¨å…±äº«å†…å­˜ä¸­
- å°†å½’çº¦è½´ `K` åˆ†å‰²æˆç“¦ç‰‡
- ä½¿ç”¨å¤šä¸ªçº¿ç¨‹åä½œåŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜
- ä»¥ 16Ã—16 å…ƒç´ çš„å—å¤„ç†æ•°æ®

```py
# ä¼˜åŒ– v3ï¼šv2 + ç¼“å­˜ï¼ˆå¤šçº¿ç¨‹ï¼‰
def make_gemm_gpu_scheduler_v3(M, K, N, verbose=True):
    k, s, A, B, C = base_declaration(M, K, N)
    block_x, block_y = 16, 16
    xo, xi = s[C].split(C.op.axis[0], factor=block_x)
    yo, yi = s[C].split(C.op.axis[1], factor=block_y)

    # åˆ†å‰² k
    tile_k = 8
    ko, ki = s[C].split(k, factor=tile_k)

    s[C].bind(xo, te.thread_axis("blockIdx.x"))
    s[C].bind(yo, te.thread_axis("blockIdx.y"))
    s[C].bind(xi, te.thread_axis("threadIdx.x"))
    s[C].bind(yi, te.thread_axis("threadIdx.y"))

    AA = s.cache_read(A, "shared", [C])
    BB = s.cache_read(B, "shared", [C])

    s[AA].compute_at(s[C], ko)
    s[BB].compute_at(s[C], ko)

    # å¤šçº¿ç¨‹åŠ è½½æ•°æ®
    # è¿™å¤§å¤§æé«˜äº†æ€§èƒ½ï¼
    AAxi, AAyi = s[AA].split(s[AA].op.axis[0], nparts=block_x)
    AAxx, AAxy = s[AA].split(s[AA].op.axis[1], nparts=block_y)
    s[AA].bind(AAxi, te.thread_axis("threadIdx.x"))
    s[AA].bind(AAxx, te.thread_axis("threadIdx.y"))

    BBxi, BByi = s[BB].split(s[BB].op.axis[0], nparts=block_x)
    BBxx, BBxy = s[BB].split(s[BB].op.axis[1], nparts=block_y)
    s[BB].bind(BBxi, te.thread_axis("threadIdx.x"))
    s[BB].bind(BBxx, te.thread_axis("threadIdx.y"))

    if verbose:
        print("=" * 100)
        print(tvm.lower(s, [A, B, C], simple_mode=True))
        print("=" * 100)

    return s, A, B, C
```

IRï¼š

```py
@T.prim_func
def main(A: T.Buffer((1024, 2048), "float32"), B: T.Buffer((2048, 512), "float32"), C: T.Buffer((1024, 512), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 64)
    A_shared = T.allocate([128], "float32", "shared")
    B_shared = T.allocate([128], "float32", "shared")
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    blockIdx_y = T.launch_thread("blockIdx.y", 32)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    C_1 = T.Buffer((524288,), data=C.data)
    C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] = T.float32(0)
    for k_outer in range(256):
        A_shared_1 = T.Buffer((128,), data=A_shared, scope="shared")
        # å¹¶è¡ŒåŠ è½½æ•°æ®åˆ° A_shared_1
        with T.launch_thread("threadIdx.x", 16) as threadIdx_x_1:
            threadIdx_y_1 = T.launch_thread("threadIdx.y", 16)
            if T.likely(threadIdx_y_1 < 8):
                A_1 = T.Buffer((2097152,), data=A.data)
                A_shared_1[threadIdx_x_1 * 8 + threadIdx_y_1] = A_1[blockIdx_x * 32768 + threadIdx_x_1 * 2048 + k_outer * 8 + threadIdx_y_1]
        # å¹¶è¡ŒåŠ è½½æ•°æ®åˆ° B_shared_1
        B_shared_1 = T.Buffer((128,), data=B_shared, scope="shared")
        with T.launch_thread("threadIdx.x", 16) as threadIdx_x_1:
            threadIdx_y_1 = T.launch_thread("threadIdx.y", 16)
            if T.likely(threadIdx_x_1 < 8):
                B_1 = T.Buffer((1048576,), data=B.data)
                B_shared_1[threadIdx_x_1 * 16 + threadIdx_y_1] = B_1[k_outer * 4096 + threadIdx_x_1 * 512 + blockIdx_y * 16 + threadIdx_y_1]
        for k_inner in range(8):
            # blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y: è¾“å‡º
            # threadIdx_x * 8 + k_inner: A1
            # k_inner * 16 + threadIdx_y: B1
            C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] = C_1[blockIdx_x * 8192 + threadIdx_x * 512 + blockIdx_y * 16 + threadIdx_y] + A_shared_1[threadIdx_x * 8 + k_inner] * B_shared_1[k_inner * 16 + threadIdx_y]
```

æ­£å¦‚æˆ‘ä»¬åœ¨ IR ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ `A_shared_1` å’Œ `B_shared_1` å°†ç“¦ç‰‡ä¿å­˜åœ¨ç‰‡ä¸Šå†…å­˜ä¸­ï¼Œå‡å°‘è®¿é—®å…¨å±€å†…å­˜çš„æ—¶é—´æ¶ˆè€—ã€‚

è¿™å¤§å¹…æå‡äº†æ€§èƒ½è‡³ **8.11 æ¯«ç§’**ã€‚

### 2.2 AutoTVM ä¼˜åŒ–

AutoTVM å®ç°æ¢ç´¢äº†ä¸€ä¸ªæœç´¢ç©ºé—´ï¼ŒåŒ…æ‹¬ï¼š

- xã€y è½´çš„ä¸åŒç“¦ç‰‡å¤§å°ï¼ˆ8ã€16 æˆ– 32ï¼‰
- å½’çº¦è½´çš„ä¸åŒç“¦ç‰‡å¤§å°ï¼ˆ8 æˆ– 16ï¼‰
- æ˜¯å¦å‘é‡åŒ–å†…å­˜è®¿é—®
- ç¼“å­˜å†™å…¥åˆ°æœ¬åœ°å†…å­˜
- ç¼“å­˜è¯»å–åˆ°å…±äº«å†…å­˜

åœ¨æ¢ç´¢äº† 36 ç§ä¸åŒé…ç½®åï¼ŒAutoTVM è°ƒä¼˜å™¨æ‰¾åˆ°äº†ä¸€ä¸ªè¿è¡Œæ—¶é—´ä¸º **42.56 æ¯«ç§’**çš„è§£å†³æ–¹æ¡ˆã€‚

>æ³¨æ„ï¼šå¦‚æœæˆ‘ä»¬åœ¨æ›´å¤§çš„ç©ºé—´ä¸­æœç´¢ï¼ˆé«˜æˆæœ¬æ¢ç´¢ï¼‰ï¼ŒColab æ€»æ˜¯ä¼šå´©æºƒã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œåªæœç´¢ä¸€ä¸ªå°ç©ºé—´ã€‚

### 2.3 æ€§èƒ½ç»“æœ

ä»¥ä¸‹æ˜¯åœ¨ GPU ä¸Šå¯¹ `M=1024`ã€`K=2048`ã€`N=512` çš„çŸ©é˜µä¹˜æ³•çš„æ‰€æœ‰è®¡æ—¶ï¼ˆæ¯«ç§’ï¼‰ï¼š

| **å®ç°**              | **æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰** | **ç›¸å¯¹æœ´ç´ åŸºå‡†çš„åŠ é€Ÿæ¯”** | **ç›¸å¯¹å‰ä¸€ç‰ˆæœ¬çš„åŠ é€Ÿæ¯”** |
|----------------------|----------------|------------------------|------------------------|
| **æœ´ç´ åŸºå‡†**           | 84.52          | 1.0Ã—                   | -                      |
| **v1**ï¼ˆä¸€ç»´çº¿ç¨‹ï¼‰     | 36.98          | 2.3Ã—                   | 2.3Ã—                   |
| **v2**ï¼ˆäºŒç»´çº¿ç¨‹ï¼‰     | 35.50          | 2.4Ã—                   | 1.04Ã—                  |
| **v3**ï¼ˆå…±äº«å†…å­˜ï¼‰     | 8.11           | 10.4Ã—                  | 4.4Ã—                   |
| **AutoTVM**          | 42.56          | 2.0Ã—                   | -                      |
| **NumPy**ï¼ˆCPUï¼‰      | 74.95          | 1.1Ã—                   | -                      |
| **PyTorch CPU**      | 18.74          | 4.5Ã—                   | -                      |
| **PyTorch CUDA**     | 0.70           | 120.7Ã—                 | -                      |

æ‰‹åŠ¨ä¼˜åŒ– v3 é€šè¿‡åˆ©ç”¨ GPU ç‰¹å®šçš„ä¼˜åŒ–ï¼ˆå¦‚å…±äº«å†…å­˜ã€åˆ†å—å’Œåä½œçº¿ç¨‹åŠ è½½ï¼‰å®ç°äº†æ¯”æœ´ç´ åŸºå‡† **10.4 å€çš„åŠ é€Ÿ**ã€‚

ä»æœ´ç´ åˆ°ä¼˜åŒ–çš„è¿›å±•æ˜¾ç¤ºäº†ä»¥ä¸‹å‡ ç‚¹çš„é‡è¦æ€§ï¼š

- æœ‰æ•ˆçš„çº¿ç¨‹ç»„ç»‡
- é€‚å½“çš„å†…å­˜å±‚æ¬¡ç»“æ„åˆ©ç”¨
- åä½œæ•°æ®åŠ è½½

è¿™äº›åŸåˆ™æ˜¯åœ¨ GPU çŸ©é˜µä¹˜æ³•å®ç°ä¸­å®ç°é«˜æ€§èƒ½çš„åŸºç¡€ã€‚

## 3. é™„å½•

- ç¬”è®°æœ¬ï¼ˆæ­¤åšå®¢ä½¿ç”¨çš„æ‰€æœ‰ä»£ç ï¼‰ï¼š[é“¾æ¥](https://github.com/yewentao256/TVM_tutorial)
- TVM è®ºæ–‡æ‘˜è¦ï¼š[é“¾æ¥](../../TVM)
