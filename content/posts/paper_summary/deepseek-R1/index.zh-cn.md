---
title: "Summary: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
date: 2025-04-13T10:20:05+08:00
categories: ["paper_summary"]
summary: "论文速览： 'Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'"
---

> 本博客使用`o1`翻译，如有冲突请优先参考英文原文

## 下载论文

[论文](https://arxiv.org/pdf/2501.12948)

## 1. 这篇论文讲了什么？

- 本文探讨了通过**强化学习（RL）**开发推理模型，特别聚焦于 DeepSeek-R1 和 DeepSeek-R1-Zero 模型。

- 研究了如何通过**大规模RL**来提升LLM的推理能力，而不依赖传统的**监督微调（SFT）**。

- 本文还探讨了将推理模型**蒸馏**为更小、更高效的模型，同时保持较高的性能。

- 评估了 DeepSeek-R1 在多个推理任务上的表现，并将其与 OpenAI-o1 和 GPT-4o 等其他领先模型进行比较。

## 2. 与以往工作相比，这篇论文有什么创新？

- 与以往依赖大量 SFT 的工作不同，本文首次介绍了通过纯 **RL** 来增强推理能力，而不依赖监督数据，尤其是在 DeepSeek-R1-Zero 中。

- DeepSeek-R1 在应用 RL 之前，结合了少量的**冷启动数据**，解决了 DeepSeek-R1-Zero 中存在的可读性差和语言混合等问题。

- 论文展示了如何将推理能力从更大的模型（如 DeepSeek-R1）**蒸馏**到更小的模型中，在像 DeepSeek-R1-Distill-Qwen-7B 这样紧凑的模型中也能实现具有竞争力的性能。

- 通过引入**多数投票**等技术，论文展示了如何显著提升模型性能，例如将 AIME 2024 的表现从 71.0% 提升到 86.7%。

## 3. 本文进行的实验支持了哪些论点？

- 评估了 DeepSeek-R1 及其变种（DeepSeek-R1-Zero、DeepSeek-R1-Distill）在多个推理基准上的表现，包括 **MMLU**、**AIME 2024**、**Codeforces**、**LiveCodeBench** 等。

- 对蒸馏模型，如 DeepSeek-R1-Distill-Qwen-1.5B 和 DeepSeek-R1-Distill-Qwen-7B，进行了与 OpenAI-o1 和 GPT-4o 等较大模型的性能对比。

- 跟踪了 DeepSeek-R1-Zero 在 RL 训练过程中的表现，展示了它在多项任务中的进展和提升。

- 比较了**多数投票**（共识）对性能的影响，展示了这一技术如何在 AIME 2024 等基准测试中提升结果。

## 4. 这篇论文的不足/局限性是什么？

- 尽管有所改进，DeepSeek-R1 仍然存在**语言混合问题**，特别是在处理非英语或非中文的查询时。

- 大规模 RL 训练推理任务需要巨大的计算资源，且对于较小的模型而言，可能并不总是可行。

- 由于 RL 过程的长时间评估，模型在软件工程基准上的表现没有显著超越 DeepSeek-V3。

- 论文承认在使用奖励模型时可能会出现**奖励破解**的问题，这会导致训练效果不理想。

- 模型对提示格式和类型比较敏感，使用少量样本提示时，可能会降低其表现。

## 5. 构建这篇论文的合理下一步是什么？

- **解决语言混合问题**：通过增强模型的多语言能力，特别是在处理不常见语言的查询时，来解决语言混合问题。

- **提高RL效率**：研究如何使大规模 RL 训练更加**高效**，比如引入**异步评估**或其他训练策略来加速过程。

- **提升软件工程任务的性能**：通过**拒绝采样**或为工程特定领域提供更多有针对性的 RL 数据，来提升模型在软件工程任务上的表现。

- **将RL与SFT更好地结合**：将 RL 与 SFT 结合，通过 RL 来优化推理能力，同时使用 SFT 来保持模型在通用任务上的能力。

- **探索不同的提示技术**：尝试不同类型的提示方法和架构，减少对提示格式的敏感性，增强模型在实际应用中的鲁棒性。

## 附录

- **冷启动数据（Cold-Start Data）**：用于稳定强化学习（RL）训练初期阶段的初始数据。

- **多数投票（Majority Voting）**：通过聚合多个输出的响应，选择最常见的答案来提高性能的方法。

- **MMLU（大规模多任务语言理解，Massive Multitask Language Understanding）**：测试模型在多个任务上的通用语言理解能力的基准。

- **AIME 2024（美国数学邀请赛 2024）**：用于测试数学推理能力的数学竞赛基准。

- **Codeforces（编程竞赛平台）**：评估模型解决编程问题能力的在线平台。

- **LiveCodeBench（实时编程基准测试）**：评估软件工程任务解决能力的基准。

- **奖励破解（Reward Hacking）**：在强化学习过程中，模型通过操控奖励系统来获得高分，而不真正解决任务。

- **监督微调（Supervised Fine-Tuning，SFT）**：对预训练模型进行进一步训练，使其在特定任务上表现更好。

- **强化学习（Reinforcement Learning，RL）**：一种通过与环境互动并获得奖励来学习的机器学习方法。
